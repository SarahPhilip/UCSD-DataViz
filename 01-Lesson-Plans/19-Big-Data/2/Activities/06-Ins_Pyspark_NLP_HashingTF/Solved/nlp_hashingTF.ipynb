{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('hashing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------+\n",
      "|id |words                            |\n",
      "+---+---------------------------------+\n",
      "|0  |The cow cow jumped and jumped cow|\n",
      "|1  |then the cow said                |\n",
      "|2  |I am a cow that jumped           |\n",
      "+---+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample dataframe with repeating words\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"The cow cow jumped and jumped cow\"),\n",
    "    (1, \"then the cow said\"),\n",
    "    (2, \"I am a cow that jumped\")\n",
    "],[\"id\", \"words\"])\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------+-----------------------------------------+\n",
      "|id |words                            |tokens                                   |\n",
      "+---+---------------------------------+-----------------------------------------+\n",
      "|0  |The cow cow jumped and jumped cow|[the, cow, cow, jumped, and, jumped, cow]|\n",
      "|1  |then the cow said                |[then, the, cow, said]                   |\n",
      "|2  |I am a cow that jumped           |[i, am, a, cow, that, jumped]            |\n",
      "+---+---------------------------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(inputCol=\"words\", outputCol=\"tokens\")\n",
    "tokened_df = tokenizer.transform(df)\n",
    "tokened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------+-----------------------------------------+-------------------------------+\n",
      "|id |words                            |tokens                                   |filtered                       |\n",
      "+---+---------------------------------+-----------------------------------------+-------------------------------+\n",
      "|0  |The cow cow jumped and jumped cow|[the, cow, cow, jumped, and, jumped, cow]|[cow, cow, jumped, jumped, cow]|\n",
      "|1  |then the cow said                |[then, the, cow, said]                   |[cow, said]                    |\n",
      "|2  |I am a cow that jumped           |[i, am, a, cow, that, jumped]            |[cow, jumped]                  |\n",
      "+---+---------------------------------+-----------------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "stripped_df = remover.transform(tokened_df)\n",
    "stripped_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------------------+\n",
      "|filtered                       |hashed                |\n",
      "+-------------------------------+----------------------+\n",
      "|[cow, cow, jumped, jumped, cow]|(16,[11,15],[2.0,3.0])|\n",
      "|[cow, said]                    |(16,[0,15],[1.0,1.0]) |\n",
      "|[cow, jumped]                  |(16,[11,15],[1.0,1.0])|\n",
      "+-------------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# Run the hashing term frequency\n",
    "hashing = HashingTF(inputCol=\"filtered\", outputCol=\"hashed\", numFeatures=pow(2,4))\n",
    "hashed_df = hashing.transform(stripped_df)\n",
    "hashed_df.select('filtered', 'hashed').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+--------------------------------------+\n",
      "|words                            |features                              |\n",
      "+---------------------------------+--------------------------------------+\n",
      "|The cow cow jumped and jumped cow|(16,[11,15],[0.5753641449035617,0.0]) |\n",
      "|then the cow said                |(16,[0,15],[0.6931471805599453,0.0])  |\n",
      "|I am a cow that jumped           |(16,[11,15],[0.28768207245178085,0.0])|\n",
      "+---------------------------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# Fit the IDF on the data set \n",
    "idf = IDF(inputCol=\"hashed\", outputCol=\"features\")\n",
    "idf_model = idf.fit(hashed_df)\n",
    "idf_df = idf_model.transform(hashed_df)\n",
    "idf_df.select('words', 'features').show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
